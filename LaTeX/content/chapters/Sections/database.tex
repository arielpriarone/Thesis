
\section{Database}
\label{sec:Database}
In the previous \autoref{sec:commissioning}, the setup behavior phases of the framework have been described, referring to a generic \quoted{database}, without specifying the structure of the database. Let's now address the problem of storing the data efficiently and effectively. Instead of relying on \texttt{python} data structures, it is better to use a dedicated database manager.

The proposed framework uses \gls{glo:mongodb} for the following reasons. It is a widely-used, open-source \gls{glo:nosql} database that is designed to handle unstructured or semi-structured data. It utilizes a document-oriented data model, storing data in flexible, \gls{json}-like \gls{bson} format. MongoDB is suitable for implementation in a \gls{nd} framework due to its scalability, flexibility, and real-time data processing capabilities. In novelty detection, the system often deals with diverse and dynamic data sources, making MongoDB's \quoted{unstructureness} advantageous for handling varying data formats and evolving data requirements. It has the ability to handle large volumes of data and support scaling allowing for efficient storage and retrieval of information in real-time, crucial for real-time applications. Moreover, MongoDB has a rich query language and secondary indexes that allow for fast and efficient querying of data and a library for \texttt{python} that makes it easy to use.
The \gls{json} format is also human-readable, which makes it easy to understand the data stored in the database, and \quoted{mongoDB Compass} is a graphical user interface that allows one to easily explore the database.

\subsection{Collections}
\begin{longtblr}[
  caption = {Collections contained in the \gls{glo:mongodb} database},
  label = {tab:MongoDB_collections},
  ]{
  hline{1,11} = {-}{0.08em},
      hline{2} = {-}{},
    }
  \textbf{Collection} & \textbf{Content}                                       \\
  raw                 & time-series and information about them                 \\
  unconsumed          & snapshots to be evaluated                              \\
  quarantined         & {snapshots detected as novelty waiting to be declared  \\healthy, faulty or be discarded}\\
  healthy             & snapshots declared as normal behavior                  \\
  healthy train       & {training dataset (scaled, processed, packet)          \\for the \gls{nd} \gls{uml} model}\\
  faulty              & snapshots declared as faulty behavior                  \\
  faulty train        & {training dataset (scaled, processed, packet)          \\for the \gls{nd} \gls{uml} model}\\
  models              & {models trained on healthy and faulty data the metrics \\and predictions to be shown}\\
  backup              & time-series, features, models, etc.
\end{longtblr}

MongoDB structure is based on collections, that are groups of (\gls{json}) documents. A document is a set of key-value pairs that can be nested in several layers. Documents have a dynamic schema, which means that documents in the same collection do not need to have the same set of fields or structure, and common fields in a collection's documents may hold different types of data. To store the data needed by the framework the collections reported in \autoref{tab:MongoDB_collections} are used.
In the following paragraphs, the structure and purposes of each collection are described.

\paragraph{Raw}
Thinking about the data flow, the first interface between the hardware and the software would be the sensor readings. Every sensor should have a name and be sampled at a constant frequency (or, at least, the sensors that provide data for frequency-domain feature extraction should have a constant sampling frequency). This data is stored in the {raw} collection, with the following \gls{json} structure:
\begin{lstlisting}[language=json,firstnumber=1]
    {
        "_id": {
          "$oid": "xx...xxx"
        },
        "timestamp": {
          "$date": "YYYY-MM-DDThh:mm:ss.SSSZ"
        },
        "Sensor 1": {
          "sampFreq": 123456,
          "timeSerie": [
            123.456,
            . . . ,
            123.456
          ]
        },
        ...
        "Sensor n": {
          "sampFreq": 123456,
          "timeSerie": [
            -123.456,
            . . . ,
            123.456
          ]
        }
      }
\end{lstlisting}
where \texttt{\_id} is the unique identifier of the document, \texttt{timestamp} is the time at which the data was acquired, in \gls{iso} format, and \texttt{Sensor 1} to \texttt{Sensor n} are the names of the sensors. Each sensor has a \texttt{sampFreq} field that contains the sampling frequency of that particular sensor, and a \texttt{timeSerie} field that contains the data acquired by the sensor, as a list. The \texttt{timeSerie} field is a list of floating point numbers, that can be of any length. Note that the sampling frequencies of different sensors can be different, for example, if a timestamp contains $1\si{\s}$ period of data, a vibration sensor would be linked to an array with several thousands of samples, while a temperature sensor would be linked to only one sample.

\paragraph{Unconsumed}
Once defined the structure that the time-series will have in the database, let's define the structure of the snapshots. The features extracted from the time-series are stored in the {unconsumed} collection, with the following \gls{json} structure:

\begin{lstlisting}[language=json,firstnumber=1]
    {
        "_id": {
          "$oid": "xxx....xxx"
        },
        "timestamp": {
          "$date": "YYY-MM-DDThh:mm:ss.SSSZ"
        },
        "Sensor 1": {
          "mean": 123.456,
          "rms": 123.456,
          "peak2peak": 123.456,
          "std": 123.456,
          "skewness": 123.456,
          "kurtosis": 123.456,
          "wavelet coef aaaaaa": 123.456,
          "wavelet coef aaaaad": 123.456,
          ...,
          "wavelet coef dddddd": 123.456
        },
        "Sensor 2": {
          "mean": 123.456,
          "kurtosis": 1.099722740047154
        },
        ...,
        "Sensor n": {
          "mean": 123.456,
          ...
        }
        "novelty evaluated": true/false
      }
\end{lstlisting}

Notice that different sensors can have different features. The \quoted{novelty evaluated} field is a boolean that is set to \texttt{false} when the snapshot is created, and is set to \texttt{true} when the \gls{nd} algorithm evaluates the snapshot. This field is used to avoid evaluating the same snapshot multiple times while leaving it in the collection until also the \gls{fd} algorithm is performed. At this point, the snapshot will be moved either to the backup collection, discarded or to the quarantine collection if either the \gls{nd} or the \gls{fd} flag it.

\paragraph{Quarantined}
The \quoted{quarantined} collection is used to store the snapshots that were flagged as \quoted{novelty} by the \gls{nd} algorithm or as \quoted{faulty} by the \gls{fd} algorithm (or were flagged by both of them). The structure is the same as the \quoted{unconsumed} collection, but the \quoted{novelty evaluated} field is not present since, at this point, the snapshots are guaranteed to have been evaluated. The snapshots in this collection are waiting to be declared as \quoted{healthy} or \quoted{faulty} by the user or to be discarded.

\paragraph{Healthy}
The idea behind the \quoted{healthy} collection is to store the snapshots that are acquired during the first work phase of the framework, before training, or the snapshots that were in the \quoted{quarantine} collection and were declared as healthy by the user. The documents in this collection have the same structure as the documents in the \quoted{quarantined} collection.

\paragraph{Healthy train}
In this collection the healthy snapshots are packed together in different documents, each of them useful in a different phase of the training process.

  {The first document hase the \texttt{id} \texttt{training\_set}, that contains all the \texttt{N} training snapshots, each of them with \texttt{n} sensors signals, characterized by \texttt{F} features. For ease of accessibility, every bottom-nested field is a list of \texttt{N} elements. The structure is the following:}

\begin{lstlisting}[language=json,firstnumber=1]
{
  "_id": "training set",
  "timestamp": [
    {
      "$date": "YYYY-MM-DDThh:mm:ss.SSSZ" 	# timestamp 1
    },  
	...,
    {   
      "$date": "YYYY-MM-DDThh:mm:ss.SSSZ" 	# timestamp N
    }
  ],
  "Sensor 1": {
    "Feature 1": [
      123.456,								# value 1
      ...,
      123.456								# value N
    ],
	...,
	"Feature F": [
      123.456,								# value 1
      ...,
      123.456								# value N
    ]},
	...
  "Sensor n": {
    "Feature 1": [
      123.456,								# value 1
      ...,
      123.456								# value N
    ],
	...,
	"Feature F": [
      123.456,								# value 1
      ...,
      123.456								# value N
    ]}
}
\end{lstlisting}

This collection contains other three documents:
\begin{itemize}
  \item \texttt{training set scaled}, that contains the scaled training set, having the same structure as the \texttt{training set} document;
  \item \texttt{training set MIN MAX}, that contains the minimum and maximum values of the features of the training set, useful to plot the features with a reference of the bounds of the training set. It has the same structure of the \texttt{training set} document, but the bottom-nested fields are lists of two elements (the minimum and the maximum value);
  \item \texttt{StandardScaler\_pickled}. It contains the \texttt{StandardScaler} object that was used to scale the training set. This object is encoded in \gls{glo:pickle}, and it is used during the evaluation phase to scale the snapshots before evaluating them.
\end{itemize}

\paragraph{Faulty}
This collection serves the same exact purpose as the \quoted{healthy} collection, but for the faulty snapshots. Faulty snapshots are not discarded because they can be used to train the \gls{fd} \gls{uml} algorithm.

\paragraph{Faulty train}
This collection serves the same exact purpose as the \quoted{healthy train} collection, but for the faulty snapshots.

\paragraph{Models}
This collection contains the models trained on the healthy and faulty data and a buffer of the predictions and metrics to be displayed to the user.

The structure of the models documents is just an identifier and the \texttt{python} object of the model, encoded in \gls{glo:pickle}. The structure of the predictions and metrics documents is the following:

\begin{lstlisting}[language=json,firstnumber=1]
{
  "_id": "Kmeans cluster novelty indicator",
  "values": [										# novelty metric
    123.456,										# first buffer value
    ...,
    123.456									        # last buffer value
  ],
  "timestamp": [
    {
      "$date": "YYYY-MM-DDThh:mm:ss.SSSZ"			# first buffer date
    },
  ...,
    {
      "$date": "YYYY-MM-DDThh:mm:ss.SSSZ"			# last buffer date
    }
  ],
  "assigned_cluster": [								# predicted cluster
    1,												# first predicted cluster
    ...,
    4												# last predicted cluster
  ],
  "anomaly": [										# anomaly flag
    false,											# first flag
    ...,
    true											# last flag
  ],
  "pred_parameters": [								# last prediction curve 
    {
      "$binary": {
        "base64": "....",
        "subType": "00"
      }
    }
  ]
}
\end{lstlisting}

\paragraph{Backup}
The backup collection is a general-purpose container for any document that needs to be stored for backup purposes. It can contain time-series, features, models, etc. The structure of the documents in this collection is the same as the structure of the documents in the other collections.

