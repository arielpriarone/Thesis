\chapter{Unsupervsed Learning}
\label{ch:Unsupervised}

In the previous \autoref{ch:MachineLearning}, an overview of the most commmon supervised learning algorithms has been given, but all these techniques require a labeled dataset. As anticipated in the introduction, most of the time a model o the machine to be monitored is not available. Furthermore, usually a prior knowledge of the behaviour of the machine in the \emph{healthy} or a \emph{faulty} state is not available either. Even in the best case scenario, where some data collection has been done, the data will be unlabeled. 

To address this scenario, two approaches are possible: either label the data, or use an unsupervised learning algorithm. The former would be tedious and time consuming in the case the dataset contains both healthy and faulty data, because the faulty data would have to be labeled by hand. If the dataset contains only healthy data, it would be trivial to automatically label all the instances as healthy and use a supervised learning algorithm, but this would be a stretch of the definition of supervised learning. The latter is a more linear approach, since unsupervised learning algorithms are designed to work with unlabeled data.

The most common unsupervised task is dimensionality reduction \citepage{hands-on-geron2022}{260}, but in this thesis the main focus will be on novelty detection, fault detection and predictive maintenance, so the considered \gls{uml} algorithms will be clustering and density estimation.

\paragraph[]{Clustering}
Clustering is the task of grouping together similar instances. The definition of \emph{similar} depends on the algorithm used. The most common algorithms are {k-means} and \gls{dbscan}. The former is a \gls{glo:cent}-based algorithm, it is fast to evaluate a new instance and produce a light-weigth model but perform porely in some conditions that will be descri in detail. The latter is a density-based algorithm, it performs better in the condition where k-means fails, but has the drawbacks of being much slower to evaluate a new instance, and to perform novelty detection, the proposed solution has to keep all the train data in memory. Both will be described in detail in the following \autoref{sec:kmeans} and \autoref{sec:dbscan}.

\paragraph[]{Gaussian mixture models}
The second approach to novelty detection is the use of gaussian mixture models (\gls{gmm}). This approach is based on the assumption that the data is generated from a mixture of several gaussian distributions with unknown parameters \citepage{hands-on-geron2022}{283}. Then the distribution model can be used for novelty detection. This approach will be described in detail in the following \autoref{sec:gaussian}.

\paragraph{Other approaches}
At the end of the chapter, some other approaches will be briefly described and tested on the same dataset used for demostrating clustering and \gls{gmm}. These approaches are: \gls{iforest}, \gls{lof} and $\nu$-\gls{svm}. The first two are based on the assumption that outliers are instances that are isolated from the rest of the data, while the latter is based on a kernelized \gls{svm} algorithm. 

\input{content/chapters/Sections/Kmeans.tex}
\input{content/chapters/Sections/DBscan.tex}
\input{content/chapters/Sections/Gaussian.tex}
\input{content/chapters/Sections/IsolationForest.tex}
\input{content/chapters/Sections/LocalOutlierFactor.tex}
\input{content/chapters/Sections/OneClassSupportVectorMachine.tex}
